{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a276ea8c-c7f5-4a2d-b514-b70bbbed6174",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testbed 2.3.4 LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9afa17dd-9e56-4a25-a722-440b4018868b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zchua\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import time\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91c7473-7662-472f-b7ee-e843826bad9b",
   "metadata": {},
   "source": [
    "#prototype densenet\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DenseNet(nn.Module):\n",
    "    def __init__(self, growth_rate=32, block_config=(6, 12, 24, 16),\n",
    "                 num_init_features=64, bn_size=4, drop_rate=0, num_classes=10):\n",
    "        super(DenseNet, self).__init__()\n",
    "        \n",
    "        self.growth_rate = growth_rate\n",
    "        self.block_config = block_config\n",
    "        self.num_init_features = num_init_features\n",
    "        self.bn_size = bn_size\n",
    "        self.drop_rate = drop_rate\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # First Convolutional layer\n",
    "        self.features = nn.Sequential(OrderedDict([\n",
    "            ('conv0', nn.Conv2d(3, num_init_features, kernel_size=3, stride=1, padding=1, bias=False)),\n",
    "            ('norm0', nn.BatchNorm2d(num_init_features)),\n",
    "            ('relu0', nn.ReLU(inplace=True)),\n",
    "            ('pool0', nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        ]))\n",
    "        \n",
    "        # Dense blocks\n",
    "        num_features = num_init_features\n",
    "        for i, num_layers in enumerate(block_config):\n",
    "            block = DenseBlock(num_layers=num_layers, num_input_features=num_features,\n",
    "                               bn_size=bn_size, growth_rate=growth_rate, drop_rate=drop_rate)\n",
    "            self.features.add_module('denseblock%d' % (i + 1), block)\n",
    "            num_features = num_features + num_layers * growth_rate\n",
    "            if i != len(block_config) - 1:\n",
    "                trans = Transition(num_input_features=num_features, num_output_features=num_features // 2)\n",
    "                self.features.add_module('transition%d' % (i + 1), trans)\n",
    "                num_features = num_features // 2\n",
    "        \n",
    "        # Final batch norm\n",
    "        self.features.add_module('norm5', nn.BatchNorm2d(num_features))\n",
    "        \n",
    "        # Linear layer\n",
    "        self.classifier = nn.Linear(num_features, num_classes)\n",
    "        \n",
    "        # Official init from torch repo.\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = self.features(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ca147a9-fbaf-4587-9a5b-3b26cc934975",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#adding L1 and L2 reg\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.conv5 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1)\n",
    "        self.conv6 = nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(in_features=1024, out_features=512)\n",
    "        self.fc2 = nn.Linear(in_features=512, out_features=256)\n",
    "        self.fc3 = nn.Linear(in_features=256, out_features=128)\n",
    "        self.fc4 = nn.Linear(in_features=128, out_features=64)\n",
    "        self.fc5 = nn.Linear(in_features=64, out_features=10)\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.leaky_relu(self.conv1(x), negative_slope=0.1)\n",
    "        x = nn.functional.max_pool2d(x, kernel_size=2, stride=2)\n",
    "        x = nn.Dropout(p=0.1)\n",
    "        x = nn.functional.leaky_relu(self.conv2(x), negative_slope=0.1)\n",
    "        x = nn.functional.max_pool2d(x, kernel_size=2, stride=2)\n",
    "        x = nn.Dropout(p=0.2)\n",
    "        x = nn.functional.leaky_relu(self.conv3(x), negative_slope=0.1)\n",
    "        x = nn.functional.max_pool2d(x, kernel_size=2, stride=2)\n",
    "        x = nn.Dropout(p=0.3)\n",
    "        x = nn.functional.leaky_relu(self.conv4(x), negative_slope=0.1)\n",
    "        x = nn.functional.max_pool2d(x, kernel_size=2, stride=2)\n",
    "        x = nn.Dropout(p=0.4)\n",
    "        x = nn.functional.leaky_relu(self.conv5(x), negative_slope=0.1)\n",
    "        x = nn.functional.max_pool2d(x, kernel_size=2, stride=2)\n",
    "        x = nn.Dropout(p=0.5)\n",
    "        x = nn.functional.leaky_relu(self.conv6(x), negative_slope=0.1)\n",
    "        x = nn.functional.max_pool2d(x, kernel_size=2, stride=2)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = nn.functional.leaky_relu(self.fc1(x))\n",
    "        x = nn.Dropout(p=0.5)\n",
    "        x = nn.functional.leaky_relu(self.fc2(x))\n",
    "        x = nn.functional.leaky_relu(self.fc3(x))\n",
    "        x = nn.functional.leaky_relu(self.fc4(x))\n",
    "        #x = nn.functional.softmax(self.fc3(x))\n",
    "        x = self.fc5(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdc00592-8461-4f98-ac4e-12c05cd3f49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset and data loader\n",
    "transform = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
    "                                transforms.RandomRotation(10),\n",
    "                                transforms.RandomCrop(32, padding=4),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5, 0.5, 0.5), (0.2, 0.2, 0.2))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f1a5697-7c6c-4369-8fb3-887138ea420f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=100, shuffle=True, num_workers=2)\n",
    "testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0727f41d-e000-4d32-baca-de95b0aea8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function and optimizer\n",
    "model = SimpleModel()\n",
    "#loss function is used to measure the error between the predicted output of the model and the ground-truth label\n",
    "criterion = nn.CrossEntropyLoss() #LF\n",
    "optimizer1 = optim.Adam(model.parameters(), lr=0.001)\n",
    "optimizer2 = optim.Adam(model.parameters(), lr=0.0005)\n",
    "optimizer3 = optim.Adam(model.parameters(), lr=0.0002)\n",
    "lr_ep = 50\n",
    "lr_ep2 = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "704e3a23-ca8e-492e-bd5f-94aaa435c976",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adjust learning rate after x epochs\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=40, gamma=0.1) # this killed the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b065990a-3933-4447-aaca-9bec95de629d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of training epochs\n",
    "num_epochs = 300\n",
    "patience = 99 #set to 99 for testing as we want to see long term model performance \n",
    "counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75a3bf0f-6c64-4320-88d6-35afb20a353b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#TEMP load in model\n",
    "#model = SimpleModel()\n",
    "#model.load_state_dict(torch.load('leaky_relu_model2.3.3.pt'))\n",
    "#results showed loading in ReLU model did not produce great loss results - weights are not really transferrable between different loss function models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca75a227-cbeb-4c3d-a467-272a0e47700d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin model training\n",
      "Patience has been set at 99 for 300 epochs\n",
      "Learning rate set at 0.001 decreasing to 0.0005 at epoch 50\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "conv2d() received an invalid combination of arguments - got (Dropout, Parameter, Parameter, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, tuple of ints padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mDropout\u001b[0m, \u001b[31;1mParameter\u001b[0m, \u001b[31;1mParameter\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[32;1mint\u001b[0m)\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, str padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mDropout\u001b[0m, \u001b[31;1mParameter\u001b[0m, \u001b[31;1mParameter\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[32;1mint\u001b[0m)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 34\u001b[0m\n\u001b[0;32m     32\u001b[0m inputs, labels \u001b[38;5;241m=\u001b[39m data\n\u001b[0;32m     33\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 34\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#tensor produced, indecipherable when printed \u001b[39;00m\n\u001b[0;32m     35\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels) \u001b[38;5;66;03m#criterion is loss function\u001b[39;00m\n\u001b[0;32m     36\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward() \u001b[38;5;66;03m#used to calculate the gradients of the parameters of a model with respect to a loss function\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[4], line 21\u001b[0m, in \u001b[0;36mSimpleModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     19\u001b[0m x \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mmax_pool2d(x, kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     20\u001b[0m x \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mDropout(p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m---> 21\u001b[0m x \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mleaky_relu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m, negative_slope\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m     22\u001b[0m x \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mmax_pool2d(x, kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     23\u001b[0m x \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mDropout(p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: conv2d() received an invalid combination of arguments - got (Dropout, Parameter, Parameter, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, tuple of ints padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mDropout\u001b[0m, \u001b[31;1mParameter\u001b[0m, \u001b[31;1mParameter\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[32;1mint\u001b[0m)\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, str padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mDropout\u001b[0m, \u001b[31;1mParameter\u001b[0m, \u001b[31;1mParameter\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[32;1mint\u001b[0m)\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "valid_test = 99\n",
    "epoch_test = 0\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "time_list = []\n",
    "t1 = time.perf_counter()\n",
    "reset_flag = 0\n",
    "print('Begin model training')\n",
    "print('Patience has been set at {} for {} epochs'.format(patience, num_epochs)) \n",
    "print('Learning rate set at {} decreasing to {} at epoch {}'.format(optimizer1.param_groups[0][\"lr\"], optimizer2.param_groups[0][\"lr\"], lr_ep))\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0\n",
    "    test_loss = 0\n",
    "    \n",
    "    #Set learning rate\n",
    "    if lr_ep > epoch: #added flag to decrease LR further once model is reset\n",
    "        optimizer = optimizer1\n",
    "    elif lr_ep > epoch and lr_ep2 > epoch: \n",
    "        optimizer = optimizer2\n",
    "    else:\n",
    "        optimizer = optimizer3\n",
    "    \n",
    "\n",
    "    \n",
    "    #Training \n",
    "    correct_t = 0\n",
    "    total_t = 0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs) #tensor produced, indecipherable when printed \n",
    "        loss = criterion(outputs, labels) #criterion is loss function\n",
    "        loss.backward() #used to calculate the gradients of the parameters of a model with respect to a loss function\n",
    "        optimizer.step() #updates the model parameters based on the gradients computed during the backward pass of training\n",
    "                \n",
    "        _, preds = torch.max(outputs, 1) #produces tensor containing indices of the maximum values (i.e. the predicted classes)\n",
    "        correct_t += (preds == labels).sum().item()\n",
    "        train_loss += loss.item()\n",
    "        total_t += labels.size(0) #total equals 50000 by the end of this for loop for CIFAR10\n",
    "        #correct_t += (preds == labels).sum().item()\n",
    "        #print(preds==labels)\n",
    "        #print()\n",
    "    \n",
    "    #Validation\n",
    "    correct_v = 0\n",
    "    total_v = 0\n",
    "    for i, data in enumerate(test_loader, 0):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct_v += (preds == labels).sum().item()\n",
    "        test_loss += loss.item()\n",
    "        total_v += labels.size(0) #total equals 10000 by the end of this for loop for CIFAR10\n",
    "        #add correct\n",
    "   \n",
    "    \n",
    "    #defining accuracy and loss \n",
    "    t_acc = correct_t/total_t\n",
    "    v_acc = correct_v/total_v\n",
    "    new_train_lost = train_loss / len(train_loader)\n",
    "    new_valid_lost = test_loss / len(test_loader)\n",
    "    \n",
    "    #Save trained model if it is improved \n",
    "    if new_valid_lost < valid_test:\n",
    "        valid_test = new_valid_lost\n",
    "        epoch_test = epoch+1\n",
    "        torch.save(model.state_dict(), \"leaky_relu_model2.3.4.pt\")\n",
    "        counter = 0\n",
    "        print ('Temp model saved at Epoch {} with validation lost of {:.4f}'.format(epoch_test, valid_test))\n",
    "    else:\n",
    "        counter+=1\n",
    "        print('No improvement in test, count is {}'.format(counter))\n",
    "        \n",
    "    #Terminates training after model stops improving based on patience \n",
    "    if counter > patience:\n",
    "        break\n",
    "    \n",
    "    #prints stuff\n",
    "    t2 = time.perf_counter()\n",
    "    print('Training loss for Epoch {} is {:.4f} and Training accuracy is {:.2f}'.format(epoch + 1, new_train_lost, t_acc))\n",
    "    print('Validation loss for Epoch {} is {:.4f} and Validation accuracy is {:.2f}'.format(epoch + 1, new_valid_lost, v_acc))\n",
    "    print('Completed Epoch {} in {:.1f} seconds with LR of {}'.format(epoch + 1, t2-t1, optimizer.param_groups[0][\"lr\"]))\n",
    "    #print('Epoch: {} Loss: {:.4f} Train_Acc: {:.4f}'.format(epoch, train_loss / len(dataloader), running_corrects.double() / len(dataset)))\n",
    "    \n",
    "    #makes list of loss, accuract, and time for epoch\n",
    "    train_acc.append(t_acc)\n",
    "    test_acc.append(v_acc)\n",
    "    train_losses.append(new_train_lost)\n",
    "    test_losses.append(new_valid_lost)\n",
    "    time_list.append(t2-t1)\n",
    "\n",
    "print ('Final model saved at Epoch {} with validation lost of {:.4f}'.format(epoch_test, valid_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c28664b-084c-4886-8cda-afa4a809d452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write train and test lists to a CSV file\n",
    "with open('leaky_relu_model2.3.4.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Training Loss', 'Test Loss', 'Train Accuracy', 'Test Accuracy', 'Time'])\n",
    "    rows = zip(train_losses, test_losses, train_acc, test_acc, time_list)\n",
    "    writer.writerows(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57d60d7-2d86-4081-b336-89f2a730c2dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
